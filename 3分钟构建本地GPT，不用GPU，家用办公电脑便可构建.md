---
åˆ›å»ºæ—¶é—´: 2024-03-15 16:47
ä¿®æ”¹æ—¶é—´: 2024-03-15 17:09
tags:
  - python
  - AI
  - gpt
  - æ•ˆç‡å·¥å…·
æ ‡é¢˜: 3åˆ†é’Ÿæ„å»ºæœ¬åœ°GPTï¼Œä¸ç”¨GPUï¼Œå®¶ç”¨åŠå…¬ç”µè„‘ä¾¿å¯æ„å»º
share: "true"
---
[ollama/ollamaï¼šä½¿ç”¨ Llama 2ã€Mistralã€Gemma å’Œå…¶ä»–å¤§å‹è¯­è¨€æ¨¡å‹å¯åŠ¨å¹¶è¿è¡Œã€‚ (github.com)](https://github.com/ollama/ollama)
### å®‰è£… Ollama

- è½¬åˆ° Ollama ç½‘ç«™
- å•å‡»â€œä¸‹è½½â€ã€‚
- é€‰æ‹©ä¸æ‚¨çš„æ“ä½œç³»ç»ŸåŒ¹é…çš„å®‰è£…ç¨‹åºã€‚
- æŒ‰ç…§å®‰è£…å‘å¯¼è¿›è¡Œæ“ä½œã€‚

### å®‰è£…æ¨¡å‹
å®‰è£…å®Œ **Ollama** åï¼Œä½ å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ¥å®‰è£…æ¨¡å‹å¹¶è¿è¡Œï¼š

1. **å®‰è£… Ollama æ¨¡å‹**ï¼š
    
    - é¦–å…ˆï¼Œç¡®ä¿ä½ å·²ç»æˆåŠŸå®‰è£…äº† Ollamaã€‚å¦‚æœä½ è¿˜æ²¡æœ‰å®‰è£…ï¼Œè¯·å‚è€ƒä¹‹å‰çš„æ­¥éª¤ã€‚
    - æ‰“å¼€ç»ˆç«¯æˆ–å‘½ä»¤è¡Œç•Œé¢ã€‚
2. **ä¸‹è½½æ¨¡å‹**ï¼š
    
    - Ollama æä¾›äº†ä¸€ç³»åˆ—é¢„æ„å»ºçš„æ¨¡å‹ï¼Œä½ å¯ä»¥ä» Ollama æ¨¡å‹åº“ä¸­é€‰æ‹©ä¸€ä¸ªã€‚
    - ä¾‹å¦‚ï¼Œä½ å¯ä»¥ä¸‹è½½ Llama 2 æ¨¡å‹ï¼š
        
        ```bash
        ollama download llama2
        ```
        
        è¿™å°†ä¸‹è½½ Llama 2 æ¨¡å‹å¹¶å°†å…¶å®‰è£…åˆ°æœ¬åœ°ã€‚
3. **è¿è¡Œæ¨¡å‹**ï¼š
    
    - ä¸‹è½½å®Œæ¨¡å‹åï¼Œä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¥è¿è¡Œå®ƒï¼š
        
        ```bash
        ollama run llama2
        ```
        
        è¿™å°†å¯åŠ¨ Llama 2 æ¨¡å‹å¹¶ç­‰å¾…è¾“å…¥ã€‚
    - è¾“å…¥ä½ æƒ³è¦æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ï¼Œç„¶åæŒ‰ä¸‹å›è½¦é”®ã€‚
4. **è·å–æ¨¡å‹è¾“å‡º**ï¼š
    
    - æ¨¡å‹ä¼šç”Ÿæˆä¸€ä¸ªæ–‡æœ¬è¾“å‡ºï¼Œä½ å¯ä»¥åœ¨ç»ˆç«¯æˆ–å‘½ä»¤è¡Œç•Œé¢ä¸­çœ‹åˆ°ã€‚
    - å¦‚æœä½ æƒ³å°†è¾“å‡ºä¿å­˜åˆ°æ–‡ä»¶ä¸­ï¼Œå¯ä»¥ä½¿ç”¨é‡å®šå‘æ“ä½œç¬¦ `>`ï¼š
        
        ```bash
        ollama run llama2 > output.txt
        ```
        
        è¿™å°†æŠŠæ¨¡å‹çš„è¾“å‡ºä¿å­˜åˆ°åä¸º `output.txt` çš„æ–‡ä»¶ä¸­ã€‚
5. **è‡ªå®šä¹‰æ¨¡å‹å‚æ•°**ï¼ˆå¯é€‰ï¼‰ï¼š
    
    - Ollama å…è®¸ä½ è‡ªå®šä¹‰æ¨¡å‹çš„å‚æ•°ï¼Œä¾‹å¦‚æ¸©åº¦ã€ç”Ÿæˆçš„æ–‡æœ¬é•¿åº¦ç­‰ã€‚
    - æŸ¥é˜… Ollama æ–‡æ¡£ä»¥è·å–æ›´å¤šå…³äºè‡ªå®šä¹‰å‚æ•°çš„ä¿¡æ¯ã€‚

### python è°ƒç”¨
```python
import ollama

def ask_question(question):
    response = ollama.chat(model='gemma:7b', messages=[{'role': 'user', 'content': question}])
    return response['message']['content']

# ä½¿ç”¨å‡½æ•°
print(ask_question("ä½ èƒ½å¹²å—"))
```


ollama rm llama2 ä¸ºåˆ é™¤å‘½ä»¤

è¯·æ³¨æ„ï¼Œè¿™é‡Œçš„ç¤ºä¾‹æ˜¯åŸºäº Llama 2 æ¨¡å‹çš„ã€‚ä½ å¯ä»¥æ ¹æ®éœ€è¦é€‰æ‹©å…¶ä»–æ¨¡å‹ï¼Œå¹¶æŒ‰ç…§ç±»ä¼¼çš„æ­¥éª¤è¿›è¡Œæ“ä½œã€‚ç¥ä½ åœ¨ä½¿ç”¨ Ollama è¿›è¡Œæ¨¡å‹ç”Ÿæˆæ—¶ç©å¾—æ„‰å¿«ï¼ğŸš€


```
ollama pull mistral
ollama pull nomic-embed-text
# Remember to close the Ollama app first to free up the port
set OLLAMA_ORIGINS=app://obsidian.md*
ollama serve
```